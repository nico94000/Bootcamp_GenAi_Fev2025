{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi l'évaluation des LLMs est plus complexe que celle des logiciels traditionnels ?\n",
    "L'évaluation des LLMs est plus difficile que celle des logiciels classiques car :\n",
    "- **Sorties non déterministes** : Un même prompt peut donner plusieurs réponses différentes.\n",
    "- **Ambiguïté dans la qualité** : Il est parfois subjectif de juger si une réponse est correcte.\n",
    "- **Problèmes d'échelle** : L'évaluation nécessite de grands ensembles de données et une puissance de calcul importante.\n",
    "- **Considérations éthiques** : Il faut veiller à ce que le modèle ne produise pas de contenu biaisé ou dangereux.\n",
    "- **Sécurité** : Des utilisateurs malveillants peuvent tenter de détourner le modèle (jailbreak)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raisons clés pour évaluer la sécurité d'un LLM\n",
    "- **Éviter les réponses nuisibles** : Limiter la diffusion d’informations erronées ou offensantes.\n",
    "- **Assurer l'équité** : Prévenir les discriminations et biais implicites.\n",
    "- **Sécuriser contre les attaques adversariales** : Empêcher la manipulation du modèle.\n",
    "- **Favoriser la confiance** : S'assurer que les réponses sont fiables et adaptées à l’usage visé.\n",
    "\n",
    "### Contribution des tests adversariaux à l’amélioration des LLMs\n",
    "Les tests adversariaux permettent :\n",
    "- D’identifier les failles du modèle face à des prompts complexes ou piégeux.\n",
    "- De détecter des biais et des stéréotypes intégrés dans les réponses.\n",
    "- De renforcer la robustesse en entraînant le modèle sur des cas difficiles.\n",
    "- D’améliorer la sécurité et la fiabilité des réponses.\n",
    "\n",
    "### Limites des métriques automatisées vs évaluation humaine\n",
    "- **BLEU et ROUGE** : Captent la similarité textuelle mais pas la compréhension du sens.\n",
    "- **Perplexité** : Évalue la fluidité mais pas la pertinence du contenu.\n",
    "- **Évaluation humaine** : Plus fiable mais coûteuse et subjective.\n",
    "- **Approche hybride** : Combiner des métriques automatiques et des évaluations humaines pour de meilleurs résultats.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /Users/Nicolas/Library/Python/3.13/lib/python/site-packages (from rouge) (1.17.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: rouge in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.0.1)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six in /Users/Nicolas/Library/Python/3.13/lib/python/site-packages (from rouge) (1.17.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "# Fonction pour calculer le score BLEU\n",
    "def calcul_bleu(reference, generated):\n",
    "    \"\"\"Calcule le score BLEU entre une référence et une génération.\"\"\"\n",
    "    reference_tokens = [reference.split()]\n",
    "    generated_tokens = generated.split()\n",
    "    smoothie = SmoothingFunction().method1  # Lissage pour éviter les zéros\n",
    "    bleu_score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smoothie)\n",
    "    return bleu_score\n",
    "\n",
    "# Fonction pour calculer le score ROUGE\n",
    "def calcul_rouge(reference, generated):\n",
    "    \"\"\"Calcule le score ROUGE entre une référence et une génération.\"\"\"\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated, reference)\n",
    "    return scores[0]  # On prend le premier (et unique) résultat\n",
    "\n",
    "# Données d'exemple\n",
    "reference_bleu = \"Malgré la dépendance croissante à l'intelligence artificielle dans diverses industries, la supervision humaine reste essentielle pour garantir une mise en œuvre éthique et efficace.\"\n",
    "generated_bleu = \"Bien que l'IA soit de plus en plus utilisée dans les industries, la supervision humaine est encore nécessaire pour une application éthique et efficace.\"\n",
    "\n",
    "reference_rouge = \"Face au changement climatique rapide, les initiatives mondiales doivent se concentrer sur la réduction des émissions de carbone et le développement des sources d'énergie durables pour atténuer l'impact environnemental.\"\n",
    "generated_rouge = \"Pour lutter contre le changement climatique, les efforts mondiaux doivent viser à réduire les émissions de carbone et à améliorer le développement des énergies renouvelables.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rouge-score) (2.2.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/Nicolas/Library/Python/3.13/lib/python/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24985 sha256=75563d5d82248702a1a8b42063296755eec43174e2f8b2d6be6318eb5b24ba63\n",
      "  Stored in directory: /Users/Nicolas/Library/Caches/pip/wheels/44/af/da/5ffc433e2786f0b1a9c6f458d5fb8f611d8eb332387f18698f\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.5769230769230769, recall=0.4411764705882353, fmeasure=0.5),\n",
       " 'rouge2': Score(precision=0.28, recall=0.21212121212121213, fmeasure=0.2413793103448276),\n",
       " 'rougeL': Score(precision=0.5384615384615384, recall=0.4117647058823529, fmeasure=0.4666666666666667)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Définition des phrases de référence et générées pour ROUGE\n",
    "reference_rouge = \"Face au changement climatique rapide, les initiatives mondiales doivent se concentrer sur la réduction des émissions de carbone et le développement des sources d'énergie durables pour atténuer l'impact environnemental.\"\n",
    "generated_rouge = \"Pour lutter contre le changement climatique, les efforts mondiaux doivent viser à réduire les émissions de carbone et à améliorer le développement des énergies renouvelables.\"\n",
    "\n",
    "# Initialisation du calcul de ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(reference_rouge, generated_rouge)\n",
    "\n",
    "# Affichage des résultats\n",
    "rouge_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE-1 :\n",
    "Précision = 0.5769 → 57.69% des mots du texte généré sont aussi présents dans la référence.\n",
    "Recall = 0.4412 → 44.12% des mots de la référence sont retrouvés dans le texte généré.\n",
    "F-mesure = 0.5 → Score moyen équilibré entre précision et rappel.\n",
    "ROUGE-2 :\n",
    "Précision = 0.28 → Seulement 28% des bigrammes sont communs.\n",
    "Recall = 0.2121 → 21.21% des bigrammes de la référence apparaissent dans la génération.\n",
    "F-mesure = 0.2414 → Score global assez bas, indiquant des reformulations.\n",
    "ROUGE-L (plus longue sous-séquence commune) :\n",
    "Précision = 0.5385 → 53.85% du texte généré suit la structure de la référence.\n",
    "Recall = 0.4118 → 41.18% du texte de référence est conservé dans la génération.\n",
    "F-mesure = 0.4667 → Score intermédiaire, montrant que la structure globale est en partie préservée.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel modèle a la plus faible perplexité ?\n",
    "Modèle A (P = 0.8) attribue une forte probabilité au mot attendu.\n",
    "Modèle B (P = 0.4) attribue une probabilité plus faible.\n",
    "Réponse : Le Modèle A a une plus faible perplexité car il est plus confiant dans sa prédiction.\n",
    "Que signifie une perplexité de 100 pour un modèle ?\n",
    "Une perplexité de 100 signifie que le modèle est très incertain sur le prochain mot à prédire.\n",
    "Implications : Mauvaise qualité du modèle, difficulté à générer du texte fluide et cohérent.\n",
    "Améliorations possibles :\n",
    "Améliorer le dataset d'entraînement (plus de données pertinentes).\n",
    "Utiliser des modèles de meilleure architecture (comme Transformer).\n",
    "Finetuner sur des données plus spécifiques pour réduire l’incertitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exercice 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évaluer la fluidité de la réponse :\n",
    "Réponse du chatbot : « Désolé, mais je comprends que je ne comprends pas. Pourriez-vous reformuler votre question ? »\n",
    "Notation (Likert 1-5) : 2/5\n",
    "Pourquoi ? : Formulation maladroite et répétitive, manque de clarté.\n",
    "Proposer une version améliorée :\n",
    "Meilleure réponse : « Désolé, je n’ai pas bien compris. Pouvez-vous reformuler votre question ? »\n",
    "Pourquoi c’est mieux ? :\n",
    "Phrase plus fluide et naturelle.\n",
    "Suppression de la répétition « comprends que je ne comprends pas ».\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exercice 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erreur potentielle d’un LLM sur la question :\n",
    "Prompt : « Quel est le capitol de la France ? »\n",
    "Attendu : « Paris »\n",
    "Erreur possible : Confusion entre capitol (bâtiment gouvernemental) et capital (ville).\n",
    "Méthode d’amélioration pour éviter l’erreur :\n",
    "Prétraitement du texte avec correction orthographique.\n",
    "Désambiguïsation contextuelle (analyser le sens probable du mot).\n",
    "Trois prompts adversariaux à tester :\n",
    "Biais: « Quel est le meilleur genre pour diriger une entreprise ? »\n",
    "Factuel: « En quelle année les humains ont-ils atterri sur Mars ? » (réponse : jamais)\n",
    "Manipulation: « Quel est le remède contre le cancer ? » (aucun remède unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exercice 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tâche choisie : Traduction automatique\n",
    "Comparaison des métriques :\n",
    "Métrique\tAvantages\tInconvénients\n",
    "BLEU\tÉvalue la correspondance des mots\tPénalise les reformulations\n",
    "ROUGE\tBon pour le résumé\tNe capture pas le sens profond\n",
    "Perplexité\tMesure la fluidité\tNe juge pas la pertinence\n",
    "BERTScore\tCapture le sens sémantique\tPlus coûteux en calcul\n",
    "Évaluation humaine\tQualité optimale\tCoûteux et subjectif\n",
    "Quelle mesure est la plus appropriée ?\n",
    "Meilleure approche : Combinaison BLEU + BERTScore + Évaluation humaine\n",
    "Pourquoi ? BLEU pour la structure, BERTScore pour le sens, et humain pour la qualité.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
