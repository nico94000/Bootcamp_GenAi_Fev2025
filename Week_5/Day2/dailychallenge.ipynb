{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2.tar.gz (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-t1ue269h/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=8.3.0,<8.4.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "       cwd: None\n",
      "  Complete output (292 lines):\n",
      "  Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "  Collecting setuptools\n",
      "    Downloading setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "  Collecting cython<3.0,>=0.25\n",
      "    Downloading Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "  Collecting cymem<2.1.0,>=2.0.2\n",
      "    Downloading cymem-2.0.11.tar.gz (10 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "      Preparing wheel metadata: started\n",
      "      Preparing wheel metadata: finished with status 'done'\n",
      "  Collecting preshed<3.1.0,>=3.0.2\n",
      "    Downloading preshed-3.0.9-cp38-cp38-macosx_11_0_arm64.whl (131 kB)\n",
      "  Collecting murmurhash<1.1.0,>=0.28.0\n",
      "    Downloading murmurhash-1.0.12.tar.gz (13 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "      Preparing wheel metadata: started\n",
      "      Preparing wheel metadata: finished with status 'done'\n",
      "  Collecting thinc<8.4.0,>=8.3.0\n",
      "    Downloading thinc-8.3.2.tar.gz (193 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'error'\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-5r77pc2a/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "         cwd: None\n",
      "    Complete output (79 lines):\n",
      "    Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "    Collecting setuptools\n",
      "      Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "    Collecting cython<3.0,>=0.25\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "    Collecting murmurhash<1.1.0,>=1.0.2\n",
      "      Using cached murmurhash-1.0.12.tar.gz (13 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting cymem<2.1.0,>=2.0.2\n",
      "      Using cached cymem-2.0.11.tar.gz (10 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting preshed<3.1.0,>=3.0.2\n",
      "      Using cached preshed-3.0.9-cp38-cp38-macosx_11_0_arm64.whl (131 kB)\n",
      "    Collecting blis<1.1.0,>=1.0.0\n",
      "      Downloading blis-1.0.2.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-fnh8e9uy/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/8a/69/8686634ff12188a7ac5dad16472cc66f166f6d4c34babfe8bcb4ef44ab7c/blis-1.0.2.tar.gz#sha256=68df878871a9db34efd58f648e12e06806e381991bd9e70df198c33d7b259383 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-fnh8e9uy/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Downloading blis-1.0.1.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-55q124qu/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-55q124qu/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Downloading blis-1.0.0.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-j_2krhio/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/b6/0b/a73be025d991e8795626a830314984f7bda5de440e80b72a53bc316bb870/blis-1.0.0.tar.gz#sha256=9ea14649ff07457e4112c7b94605e4aeb4f2fd5a8bd57c296ff8fbd154966ede (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-j_2krhio/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "    ERROR: Could not find a version that satisfies the requirement blis<1.1.0,>=1.0.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.8, 0.0.9.dev104, 0.0.10, 0.0.12, 0.0.13, 0.0.16, 0.1.0, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2.dev0, 0.2.2, 0.2.3.dev0, 0.2.3.dev1, 0.2.3.dev2, 0.2.3.dev3, 0.2.3, 0.2.4, 0.3.1, 0.4.0.dev0, 0.4.0.dev1, 0.4.0, 0.4.1, 0.7.0, 0.7.1.dev0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.9.0, 0.9.1.dev0, 0.9.1.dev1, 0.9.1, 1.0.0a1, 1.0.0, 1.0.1, 1.0.2, 1.2.0)\n",
      "    ERROR: No matching distribution found for blis<1.1.0,>=1.0.0\n",
      "    WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "    You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "    ----------------------------------------\n",
      "  WARNING: Discarding https://files.pythonhosted.org/packages/8a/9f/b2193b69dd112a46800182e897cda2fc9c497dfdd9352a0c5ba9252cf5f0/thinc-8.3.2.tar.gz#sha256=3e8ef69eac89a601e11d47fc9e43d26ffe7ef682dcf667c94ff35ff690549aeb (from https://pypi.org/simple/thinc/) (requires-python:>=3.6). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-5r77pc2a/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\n",
      "    Downloading thinc-8.3.1.tar.gz (193 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'error'\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-8y6jvw31/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "         cwd: None\n",
      "    Complete output (79 lines):\n",
      "    Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "    Collecting setuptools\n",
      "      Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "    Collecting cython<3.0,>=0.25\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "    Collecting murmurhash<1.1.0,>=1.0.2\n",
      "      Using cached murmurhash-1.0.12.tar.gz (13 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting cymem<2.1.0,>=2.0.2\n",
      "      Using cached cymem-2.0.11.tar.gz (10 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting preshed<3.1.0,>=3.0.2\n",
      "      Using cached preshed-3.0.9-cp38-cp38-macosx_11_0_arm64.whl (131 kB)\n",
      "    Collecting blis<1.1.0,>=1.0.0\n",
      "      Using cached blis-1.0.2.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-o2i4_8zp/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/8a/69/8686634ff12188a7ac5dad16472cc66f166f6d4c34babfe8bcb4ef44ab7c/blis-1.0.2.tar.gz#sha256=68df878871a9db34efd58f648e12e06806e381991bd9e70df198c33d7b259383 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-o2i4_8zp/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.1.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-acqrgu1l/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-acqrgu1l/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.0.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-0hcvc3y7/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/b6/0b/a73be025d991e8795626a830314984f7bda5de440e80b72a53bc316bb870/blis-1.0.0.tar.gz#sha256=9ea14649ff07457e4112c7b94605e4aeb4f2fd5a8bd57c296ff8fbd154966ede (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-0hcvc3y7/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "    ERROR: Could not find a version that satisfies the requirement blis<1.1.0,>=1.0.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.8, 0.0.9.dev104, 0.0.10, 0.0.12, 0.0.13, 0.0.16, 0.1.0, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2.dev0, 0.2.2, 0.2.3.dev0, 0.2.3.dev1, 0.2.3.dev2, 0.2.3.dev3, 0.2.3, 0.2.4, 0.3.1, 0.4.0.dev0, 0.4.0.dev1, 0.4.0, 0.4.1, 0.7.0, 0.7.1.dev0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.9.0, 0.9.1.dev0, 0.9.1.dev1, 0.9.1, 1.0.0a1, 1.0.0, 1.0.1, 1.0.2, 1.2.0)\n",
      "    ERROR: No matching distribution found for blis<1.1.0,>=1.0.0\n",
      "    WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "    You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "    ----------------------------------------\n",
      "  WARNING: Discarding https://files.pythonhosted.org/packages/ea/60/b7d645d621a47d649975b53c13cdf3e66b456a24727ccd34794f1014f45c/thinc-8.3.1.tar.gz#sha256=44e747bbf93e981dfded7d432b68ef1ba75b28ef46fc51f185477970743b36e9 (from https://pypi.org/simple/thinc/) (requires-python:>=3.6). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-8y6jvw31/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\n",
      "    Downloading thinc-8.3.0.tar.gz (193 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'error'\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-j5npjr1q/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "         cwd: None\n",
      "    Complete output (79 lines):\n",
      "    Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "    Collecting setuptools\n",
      "      Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "    Collecting cython<3.0,>=0.25\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "    Collecting murmurhash<1.1.0,>=1.0.2\n",
      "      Using cached murmurhash-1.0.12.tar.gz (13 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting cymem<2.1.0,>=2.0.2\n",
      "      Using cached cymem-2.0.11.tar.gz (10 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting preshed<3.1.0,>=3.0.2\n",
      "      Using cached preshed-3.0.9-cp38-cp38-macosx_11_0_arm64.whl (131 kB)\n",
      "    Collecting blis<1.1.0,>=1.0.0\n",
      "      Using cached blis-1.0.2.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-vgca2erz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/8a/69/8686634ff12188a7ac5dad16472cc66f166f6d4c34babfe8bcb4ef44ab7c/blis-1.0.2.tar.gz#sha256=68df878871a9db34efd58f648e12e06806e381991bd9e70df198c33d7b259383 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-vgca2erz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.1.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-7ii3kppd/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-7ii3kppd/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.0.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-61mmcbi1/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/b6/0b/a73be025d991e8795626a830314984f7bda5de440e80b72a53bc316bb870/blis-1.0.0.tar.gz#sha256=9ea14649ff07457e4112c7b94605e4aeb4f2fd5a8bd57c296ff8fbd154966ede (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-61mmcbi1/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "    ERROR: Could not find a version that satisfies the requirement blis<1.1.0,>=1.0.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.8, 0.0.9.dev104, 0.0.10, 0.0.12, 0.0.13, 0.0.16, 0.1.0, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2.dev0, 0.2.2, 0.2.3.dev0, 0.2.3.dev1, 0.2.3.dev2, 0.2.3.dev3, 0.2.3, 0.2.4, 0.3.1, 0.4.0.dev0, 0.4.0.dev1, 0.4.0, 0.4.1, 0.7.0, 0.7.1.dev0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.9.0, 0.9.1.dev0, 0.9.1.dev1, 0.9.1, 1.0.0a1, 1.0.0, 1.0.1, 1.0.2, 1.2.0)\n",
      "    ERROR: No matching distribution found for blis<1.1.0,>=1.0.0\n",
      "    WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "    You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "    ----------------------------------------\n",
      "  WARNING: Discarding https://files.pythonhosted.org/packages/15/0a/250f7fa34632616bb4fc37decbc46ed09243bade708968dc869d8a4c144b/thinc-8.3.0.tar.gz#sha256=eb3bed54f5c00ec9addaaa208c51ccfa059483d73140cd515aa33373715c6e59 (from https://pypi.org/simple/thinc/) (requires-python:>=3.6). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-j5npjr1q/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\n",
      "  ERROR: Could not find a version that satisfies the requirement thinc<8.4.0,>=8.3.0 (from versions: 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.41, 1.42, 1.60, 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.70, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 2.0, 3.0, 3.1, 3.2, 3.3, 3.4.1, 4.0.0, 4.1.0, 4.2.0, 5.0.0, 5.0.1, 5.0.2, 5.0.3, 5.0.4, 5.0.5, 5.0.6, 5.0.7, 5.0.8, 6.0.0, 6.1.0, 6.1.1, 6.1.2, 6.1.3, 6.2.0, 6.3.0, 6.4.0, 6.5.0, 6.5.2, 6.6.0, 6.7.0, 6.7.1, 6.7.2, 6.7.3, 6.8.0, 6.8.1, 6.8.2, 6.9.0, 6.10.0, 6.10.1.dev0, 6.10.1, 6.10.2.dev0, 6.10.2.dev1, 6.10.2, 6.10.3.dev0, 6.10.3.dev1, 6.10.3, 6.10.4.dev0, 6.11.0.dev2, 6.11.1.dev0, 6.11.1.dev1, 6.11.1.dev2, 6.11.1.dev3, 6.11.1.dev4, 6.11.1.dev6, 6.11.1.dev7, 6.11.1.dev10, 6.11.1.dev11, 6.11.1.dev12, 6.11.1.dev13, 6.11.1.dev15, 6.11.1.dev16, 6.11.1.dev17, 6.11.1.dev18, 6.11.1.dev19, 6.11.1.dev20, 6.11.1, 6.11.2.dev0, 6.11.2, 6.11.3.dev1, 6.11.3.dev2, 6.12.0, 6.12.1, 7.0.0.dev0, 7.0.0.dev1, 7.0.0.dev2, 7.0.0.dev3, 7.0.0.dev4, 7.0.0.dev5, 7.0.0.dev6, 7.0.0.dev8, 7.0.0, 7.0.1.dev0, 7.0.1.dev1, 7.0.1.dev2, 7.0.1, 7.0.2, 7.0.3, 7.0.4.dev0, 7.0.4, 7.0.5.dev0, 7.0.5, 7.0.6, 7.0.7, 7.0.8, 7.1.0.dev0, 7.1.0, 7.1.1, 7.2.0.dev3, 7.2.0, 7.3.0.dev0, 7.3.0, 7.3.1, 7.4.0.dev0, 7.4.0.dev1, 7.4.0.dev2, 7.4.0, 7.4.1, 7.4.2, 7.4.3, 7.4.4, 7.4.5, 7.4.6, 8.0.0.dev0, 8.0.0.dev2, 8.0.0.dev4, 8.0.0a0, 8.0.0a1, 8.0.0a2, 8.0.0a3, 8.0.0a6, 8.0.0a8, 8.0.0a9, 8.0.0a11, 8.0.0a12, 8.0.0a13, 8.0.0a14, 8.0.0a16, 8.0.0a17, 8.0.0a18, 8.0.0a19, 8.0.0a20, 8.0.0a21, 8.0.0a22, 8.0.0a23, 8.0.0a24, 8.0.0a25, 8.0.0a26, 8.0.0a27, 8.0.0a28, 8.0.0a29, 8.0.0a30, 8.0.0a31, 8.0.0a32, 8.0.0a33, 8.0.0a34, 8.0.0a35, 8.0.0a36, 8.0.0a40, 8.0.0a41, 8.0.0a42, 8.0.0a43, 8.0.0a44, 8.0.0rc0, 8.0.0rc1, 8.0.0rc2, 8.0.0rc3, 8.0.0rc4, 8.0.0rc5, 8.0.0rc6.dev0, 8.0.0rc6, 8.0.0, 8.0.1, 8.0.2, 8.0.3, 8.0.4, 8.0.5, 8.0.6, 8.0.7, 8.0.8, 8.0.9, 8.0.10, 8.0.11, 8.0.12, 8.0.13, 8.0.14.dev0, 8.0.14, 8.0.15, 8.0.16, 8.0.17, 8.1.0.dev0, 8.1.0.dev1, 8.1.0.dev2, 8.1.0.dev3, 8.1.0, 8.1.1, 8.1.2, 8.1.3, 8.1.4, 8.1.5, 8.1.6, 8.1.7, 8.1.8, 8.1.9, 8.1.10, 8.1.11, 8.1.12, 8.2.0, 8.2.1, 8.2.2, 8.2.3, 8.2.4, 8.2.5, 8.3.0, 8.3.1, 8.3.2, 9.0.0.dev0, 9.0.0.dev1, 9.0.0.dev2, 9.0.0.dev3, 9.0.0.dev4, 9.0.0.dev5)\n",
      "  ERROR: No matching distribution found for thinc<8.4.0,>=8.3.0\n",
      "  WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "  You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/07/53/536941af8fbb5cb10a778f0dbd2a17b0f13e7ebfc11e67b154be60508fdf/spacy-3.8.2.tar.gz#sha256=4b37ebd25ada4059b0dc9e0893e70dde5df83485329a068ef04580e70892a65d (from https://pypi.org/simple/spacy/) (requires-python:>=3.7). Command errored out with exit status 1: /Users/Nicolas/.pyenv/versions/3.8.10/bin/python /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-standalone-pip-9s5t6qzn/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/4x/y19tv9t12q123fdsq8qvdq_c0000gn/T/pip-build-env-t1ue269h/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=8.3.0,<8.4.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\u001b[0m\n",
      "  Downloading spacy-3.7.5-cp38-cp38-macosx_11_0_arm64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.5.tar.gz (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 117.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (56.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (24.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[K     |████████████████████████████████| 431 kB 56.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp38-cp38-macosx_15_0_arm64.whl\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 145.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 18.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (2.32.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp38-cp38-macosx_11_0_arm64.whl (489 kB)\n",
      "\u001b[K     |████████████████████████████████| 489 kB 161.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.9-cp38-cp38-macosx_11_0_arm64.whl (131 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.12-cp38-cp38-macosx_15_0_arm64.whl\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 148.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (1.24.3)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (4.67.1)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 62.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp38-cp38-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 82.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp38-cp38-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 158.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=4.12.2\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.11-cp38-cp38-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[K     |████████████████████████████████| 242 kB 96.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 32.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Building wheels for collected packages: thinc\n",
      "  Building wheel for thinc (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for thinc: filename=thinc-8.2.5-cp38-cp38-macosx_15_0_arm64.whl size=776682 sha256=f7be077038e9216b327528d93dae6b3aff42b0354b2c4c44c4852fefed149f49\n",
      "  Stored in directory: /Users/Nicolas/Library/Caches/pip/wheels/ca/65/2f/a856a5c5a2338dcc54a8e8fbbd904aef94510ff7575effd769\n",
      "Successfully built thinc\n",
      "Installing collected packages: typing-extensions, mdurl, pydantic-core, markdown-it-py, catalogue, annotated-types, srsly, shellingham, rich, pydantic, murmurhash, marisa-trie, cymem, wasabi, typer, smart-open, preshed, language-data, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, jinja2, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.5.0\n",
      "    Uninstalling typing-extensions-4.5.0:\n",
      "      Successfully uninstalled typing-extensions-4.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\u001b[0m\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 jinja2-3.1.6 langcodes-3.4.1 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.15.2 typing-extensions-4.12.2 wasabi-1.1.3 weasel-0.4.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: setuptools in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (56.0.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: jinja2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from jinja2->spacy) (2.1.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
      "Requirement already satisfied: setuptools in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (56.0.0)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: jinja2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp38-cp38-macosx_11_0_arm64.whl (168 kB)\n",
      "\u001b[K     |████████████████████████████████| 168 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from wordcloud) (3.7.5)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (4.56.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (6.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from matplotlib->wordcloud) (3.1.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Nicolas/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordcloud est bien installé !\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "print(\"wordcloud est bien installé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ alice_wonderland téléchargé avec succès !\n",
      "✅ alice_mirror téléchargé avec succès !\n",
      "✅ tangled_tale téléchargé avec succès !\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Caterpillar\n",
      " CHAPTER VI.    Pig and Pepper\n",
      " CHAPTER VII.   A Mad Tea-Party\n",
      " CHAPTER VIII.  The Queen’s Croquet-Ground\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URLs des livres\n",
    "urls = {\n",
    "    \"alice_wonderland\": \"https://www.gutenberg.org/files/11/11-0.txt\",\n",
    "    \"alice_mirror\": \"https://www.gutenberg.org/files/12/12-0.txt\",\n",
    "    \"tangled_tale\": \"https://www.gutenberg.org/files/19002/19002-0.txt\"\n",
    "}\n",
    "\n",
    "# Fonction pour télécharger et charger les textes\n",
    "def load_texts(urls):\n",
    "    texts = {}\n",
    "    for name, url in urls.items():\n",
    "        response = requests.get(url)\n",
    "        response.encoding = 'utf-8'  # Assurer un bon encodage\n",
    "        if response.status_code == 200:\n",
    "            texts[name] = response.text\n",
    "            print(f\"✅ {name} téléchargé avec succès !\")\n",
    "        else:\n",
    "            print(f\"❌ Erreur de téléchargement pour {name}\")\n",
    "    return texts\n",
    "\n",
    "# Charger les textes\n",
    "texts = load_texts(urls)\n",
    "\n",
    "# Afficher les 500 premiers caractères d'Alice au pays des merveilles\n",
    "print(texts['alice_wonderland'][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Afficher un extrait après nettoyage\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Afficher un extrait après nettoyage\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-ZÀ-ÿ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer la ponctuation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer les espaces multiples\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenisation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Supprimer les mots d'arrêt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les stopwords français et anglais\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Liste des mots d'arrêt en anglais et français\n",
    "stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "\n",
    "# Fonction de nettoyage\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir en minuscules\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ÿ\\s]', '', text)  # Supprimer la ponctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supprimer les espaces multiples\n",
    "    words = word_tokenize(text)  # Tokenisation\n",
    "    words = [word for word in words if word not in stop_words]  # Supprimer les mots d'arrêt\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Nettoyer tous les textes\n",
    "cleaned_texts = {name: clean_text(text) for name, text in texts.items()}\n",
    "\n",
    "# Afficher un extrait après nettoyage\n",
    "print(cleaned_texts[\"alice_wonderland\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/Nicolas/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chargement des stopwords réussi !\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "    stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "    print(\"✅ Chargement des stopwords réussi !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Problème avec le chargement des stopwords, relance `nltk.download('stopwords')`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Afficher un extrait après nettoyage\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Afficher un extrait après nettoyage\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-ZÀ-ÿ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer la ponctuation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer les espaces multiples\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenisation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Supprimer les mots d'arrêt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les stopwords français et anglais\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Liste des mots d'arrêt en anglais et français\n",
    "stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "\n",
    "# Fonction de nettoyage\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir en minuscules\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ÿ\\s]', '', text)  # Supprimer la ponctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supprimer les espaces multiples\n",
    "    words = word_tokenize(text)  # Tokenisation\n",
    "    words = [word for word in words if word not in stop_words]  # Supprimer les mots d'arrêt\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Nettoyer tous les textes\n",
    "cleaned_texts = {name: clean_text(text) for name, text in texts.items()}\n",
    "\n",
    "# Afficher un extrait après nettoyage\n",
    "print(cleaned_texts[\"alice_wonderland\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mcleaned_texts\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))  \u001b[38;5;66;03m# Voir les noms des textes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])  \u001b[38;5;66;03m# Vérifier un extrait\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_texts' is not defined"
     ]
    }
   ],
   "source": [
    "print(list(cleaned_texts.keys()))  # Voir les noms des textes\n",
    "print(cleaned_texts[\"alice_wonderland\"][:500])  # Vérifier un extrait\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-ZÀ-ÿ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer la ponctuation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer les espaces multiples\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenisation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Supprimer les mots d'arrêt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Nettoyer tous les textes\n",
    "cleaned_texts = {name: clean_text(text) for name, text in texts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ alice_wonderland téléchargé avec succès !\n",
      "✅ alice_mirror téléchargé avec succès !\n",
      "✅ tangled_tale téléchargé avec succès !\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Caterpillar\n",
      " CHAPTER VI.    Pig and Pepper\n",
      " CHAPTER VII.   A Mad Tea-Party\n",
      " CHAPTER VIII.  The Queen’s Croquet-Ground\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URLs des livres\n",
    "urls = {\n",
    "    \"alice_wonderland\": \"https://www.gutenberg.org/files/11/11-0.txt\",\n",
    "    \"alice_mirror\": \"https://www.gutenberg.org/files/12/12-0.txt\",\n",
    "    \"tangled_tale\": \"https://www.gutenberg.org/files/19002/19002-0.txt\"\n",
    "}\n",
    "\n",
    "# Fonction pour télécharger et charger les textes\n",
    "def load_texts(urls):\n",
    "    texts = {}\n",
    "    for name, url in urls.items():\n",
    "        response = requests.get(url)\n",
    "        response.encoding = 'utf-8'  # Assurer un bon encodage\n",
    "        if response.status_code == 200:\n",
    "            texts[name] = response.text\n",
    "            print(f\"✅ {name} téléchargé avec succès !\")\n",
    "        else:\n",
    "            print(f\"❌ Erreur de téléchargement pour {name}\")\n",
    "    return texts\n",
    "\n",
    "# Charger les textes\n",
    "texts = load_texts(urls)\n",
    "\n",
    "# Vérifier un extrait d'Alice au pays des merveilles\n",
    "print(texts[\"alice_wonderland\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Vérifier un extrait après nettoyage\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m {name: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Vérifier un extrait après nettoyage\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malice_wonderland\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-ZÀ-ÿ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer la ponctuation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer les espaces multiples\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenisation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Supprimer les mots d'arrêt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Liste des mots d'arrêt en anglais et français\n",
    "stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "\n",
    "# Fonction de nettoyage du texte\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir en minuscules\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ÿ\\s]', '', text)  # Supprimer la ponctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supprimer les espaces multiples\n",
    "    words = word_tokenize(text)  # Tokenisation\n",
    "    words = [word for word in words if word not in stop_words]  # Supprimer les mots d'arrêt\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Nettoyer tous les textes\n",
    "cleaned_texts = {name: clean_text(text) for name, text in texts.items()}\n",
    "\n",
    "# Vérifier un extrait après nettoyage\n",
    "print(cleaned_texts[\"alice_wonderland\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Path: ['/Users/Nicolas/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Télécharger les stopwords (forçage)\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Vérifier où sont stockés les fichiers NLTK\n",
    "print(\"NLTK Data Path:\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Transformer les textes\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m tfidf_X \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mcleaned_texts\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Récupérer les mots et leur score TF-IDF\u001b[39;00m\n\u001b[1;32m     10\u001b[0m words \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_texts' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialiser le vectorizer TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transformer les textes\n",
    "tfidf_X = tfidf_vectorizer.fit_transform(cleaned_texts.values())\n",
    "\n",
    "# Récupérer les mots et leur score TF-IDF\n",
    "words = tfidf_vectorizer.get_feature_names_out()\n",
    "scores = tfidf_X.toarray().sum(axis=0)\n",
    "\n",
    "# Trier les mots par importance\n",
    "tfidf_scores = dict(zip(words, scores))\n",
    "sorted_tfidf = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"🔹 10 mots les plus importants selon TF-IDF :\")\n",
    "print(sorted_tfidf[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables définies dans la session: ['CountVectorizer', 'In', 'Out', 'PorterStemmer', 'TfidfVectorizer', 'WordCloud', '_', '_12', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '__vsc_ipynb_file__', '_dh', '_exit_code', '_i', '_i1', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', '_i17', '_i18', '_i19', '_i2', '_i20', '_i21', '_i3', '_i4', '_i5', '_i6', '_i7', '_i8', '_i9', '_ih', '_ii', '_iii', '_oh', 'clean_text', 'exit', 'get_ipython', 'load_texts', 'nlp', 'nltk', 'open', 'plt', 'quit', 're', 'requests', 'spacy', 'stop_words', 'stopwords', 'texts', 'tfidf_vectorizer', 'urls', 'word_tokenize']\n"
     ]
    }
   ],
   "source": [
    "print(\"Variables définies dans la session:\", dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Vérifier si 'texts' est bien disponible avant de le nettoyer\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m     cleaned_texts \u001b[38;5;241m=\u001b[39m {name: clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_texts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m a été recréé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[22], line 28\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Vérifier si 'texts' est bien disponible avant de le nettoyer\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m     cleaned_texts \u001b[38;5;241m=\u001b[39m {name: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_texts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m a été recréé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-ZÀ-ÿ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer la ponctuation\u001b[39;00m\n\u001b[1;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer les espaces multiples\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenisation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Supprimer les mots d'arrêt\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires si besoin\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Vérifier que les textes existent\n",
    "if 'texts' not in globals():\n",
    "    print(\"⚠️ 'texts' n'est pas défini ! Tu dois réexécuter la cellule de téléchargement des livres.\")\n",
    "\n",
    "# Liste des mots d'arrêt en anglais et français\n",
    "stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "\n",
    "# Fonction de nettoyage du texte\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir en minuscules\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ÿ\\s]', '', text)  # Supprimer la ponctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supprimer les espaces multiples\n",
    "    words = word_tokenize(text)  # Tokenisation\n",
    "    words = [word for word in words if word not in stop_words]  # Supprimer les mots d'arrêt\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Vérifier si 'texts' est bien disponible avant de le nettoyer\n",
    "if 'texts' in globals():\n",
    "    cleaned_texts = {name: clean_text(text) for name, text in texts.items()}\n",
    "    print(\"✅ 'cleaned_texts' a été recréé avec succès !\")\n",
    "else:\n",
    "    print(\"⚠️ 'texts' n'est toujours pas défini. Relance d'abord la cellule de téléchargement des textes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ alice_wonderland téléchargé avec succès !\n",
      "✅ alice_mirror téléchargé avec succès !\n",
      "✅ tangled_tale téléchargé avec succès !\n",
      "📄 Livres téléchargés : ['alice_wonderland', 'alice_mirror', 'tangled_tale']\n",
      "📌 Extrait d'Alice au pays des merveilles :\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Caterpillar\n",
      " CHAPTER VI.    Pig and Pepper\n",
      " CHAPTER VII.   A Mad Tea-Party\n",
      " CHAPTER VIII.  The Queen’s Croquet-Ground\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URLs des livres\n",
    "urls = {\n",
    "    \"alice_wonderland\": \"https://www.gutenberg.org/files/11/11-0.txt\",\n",
    "    \"alice_mirror\": \"https://www.gutenberg.org/files/12/12-0.txt\",\n",
    "    \"tangled_tale\": \"https://www.gutenberg.org/files/19002/19002-0.txt\"\n",
    "}\n",
    "\n",
    "# Fonction pour télécharger et stocker les textes\n",
    "def load_texts(urls):\n",
    "    texts = {}\n",
    "    for name, url in urls.items():\n",
    "        response = requests.get(url)\n",
    "        response.encoding = 'utf-8'\n",
    "        if response.status_code == 200:\n",
    "            texts[name] = response.text\n",
    "            print(f\"✅ {name} téléchargé avec succès !\")\n",
    "        else:\n",
    "            print(f\"❌ Erreur de téléchargement pour {name}\")\n",
    "    return texts\n",
    "\n",
    "# Télécharger les textes\n",
    "texts = load_texts(urls)\n",
    "\n",
    "# Vérifier si les textes sont bien téléchargés\n",
    "print(\"📄 Livres téléchargés :\", list(texts.keys()))\n",
    "print(\"📌 Extrait d'Alice au pays des merveilles :\")\n",
    "print(texts[\"alice_wonderland\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'texts' est bien défini\n"
     ]
    }
   ],
   "source": [
    "if 'texts' in globals():\n",
    "    print(\"✅ 'texts' est bien défini\")\n",
    "else:\n",
    "    print(\"❌ 'texts' n'existe pas, tu dois réexécuter la cellule précédente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m---> 24\u001b[0m     cleaned_texts \u001b[38;5;241m=\u001b[39m {name: clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_texts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m créé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Nettoyer tous les textes\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m---> 24\u001b[0m     cleaned_texts \u001b[38;5;241m=\u001b[39m {name: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_texts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m créé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-ZÀ-ÿ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer la ponctuation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Supprimer les espaces multiples\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenisation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Supprimer les mots d'arrêt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Charger les stopwords en anglais et français\n",
    "stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "\n",
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir en minuscules\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ÿ\\s]', '', text)  # Supprimer la ponctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Supprimer les espaces multiples\n",
    "    words = word_tokenize(text)  # Tokenisation\n",
    "    words = [word for word in words if word not in stop_words]  # Supprimer les mots d'arrêt\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Nettoyer tous les textes\n",
    "if 'texts' in globals():\n",
    "    cleaned_texts = {name: clean_text(text) for name, text in texts.items()}\n",
    "    print(\"✅ 'cleaned_texts' créé avec succès !\")\n",
    "else:\n",
    "    print(\"❌ 'texts' est introuvable ! Réexécute la cellule de téléchargement.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Téléchargement des ressources NLTK terminé !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Forcer le téléchargement de toutes les ressources nécessaires\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "print(\"✅ Téléchargement des ressources NLTK terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stopwords chargés avec succès !\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "try:\n",
    "    stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "    print(\"✅ Stopwords chargés avec succès !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : stopwords introuvables ! Relance `nltk.download('stopwords')`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Vérifier que texts est bien défini\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m---> 18\u001b[0m     cleaned_texts \u001b[38;5;241m=\u001b[39m {name: clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_texts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m créé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Vérifier que texts est bien défini\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m---> 18\u001b[0m     cleaned_texts \u001b[38;5;241m=\u001b[39m {name: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_texts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m créé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-ZÀ-ÿ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m---> 12\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Recharger les stopwords\n",
    "stop_words = set(stopwords.words(\"english\") + stopwords.words(\"french\"))\n",
    "\n",
    "# Fonction de nettoyage\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-ZÀ-ÿ\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Vérifier que texts est bien défini\n",
    "if 'texts' in globals():\n",
    "    cleaned_texts = {name: clean_text(text) for name, text in texts.items()}\n",
    "    print(\"✅ 'cleaned_texts' créé avec succès !\")\n",
    "else:\n",
    "    print(\"❌ 'texts' n'est pas défini. Relance la cellule de téléchargement des livres.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Téléchargement de 'punkt' terminé !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Télécharger le package 'punkt' nécessaire pour word_tokenize()\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "print(\"✅ Téléchargement de 'punkt' terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erreur : `punkt` est introuvable, relance `nltk.download('punkt')`.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    test_text = \"Bonjour, comment vas-tu aujourd'hui ?\"\n",
    "    print(\"🔹 Test de tokenization :\", word_tokenize(test_text))\n",
    "    print(\"✅ `word_tokenize` fonctionne correctement !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : `punkt` est introuvable, relance `nltk.download('punkt')`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Téléchargement de 'punkt' terminé !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "print(\"✅ Téléchargement de 'punkt' terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erreur : `punkt` est toujours introuvable.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    test_text = \"Bonjour, comment vas-tu aujourd'hui ?\"\n",
    "    print(\"🔹 Test de tokenization :\", word_tokenize(test_text))\n",
    "    print(\"✅ `word_tokenize` fonctionne correctement !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : `punkt` est toujours introuvable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/Nicolas/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'punkt' est trouvé dans : /Users/Nicolas/nltk_data/tokenizers/punkt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Chemins où NLTK cherche ses fichiers (d'après ton affichage)\n",
    "paths_to_check = [\n",
    "    \"/Users/Nicolas/nltk_data/tokenizers/punkt\",\n",
    "    \"/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data/tokenizers/punkt\",\n",
    "    \"/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data/tokenizers/punkt\"\n",
    "]\n",
    "\n",
    "# Vérifier si 'punkt' est présent dans l'un de ces chemins\n",
    "punkt_found = False\n",
    "for path in paths_to_check:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✅ 'punkt' est trouvé dans : {path}\")\n",
    "        punkt_found = True\n",
    "\n",
    "if not punkt_found:\n",
    "    print(\"❌ 'punkt' est introuvable dans les chemins connus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erreur : `punkt` est toujours introuvable.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ajouter le bon chemin à la liste de recherche\n",
    "nltk.data.path.append(\"/Users/Nicolas/nltk_data\")\n",
    "\n",
    "# Tester `word_tokenize`\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    test_text = \"Bonjour, comment vas-tu aujourd'hui ?\"\n",
    "    print(\"🔹 Test de tokenization :\", word_tokenize(test_text))\n",
    "    print(\"✅ `word_tokenize` fonctionne correctement !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : `punkt` est toujours introuvable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chemins de recherche NLTK : ['/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/Nicolas/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Définir le bon chemin pour NLTK\n",
    "nltk.data.path = [\"/Users/Nicolas/nltk_data\"] + nltk.data.path\n",
    "\n",
    "# Vérifier si NLTK prend en compte ce chemin\n",
    "print(\"✅ Chemins de recherche NLTK :\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Charger manuellement punkt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/english.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Tester `word_tokenize`\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:823\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    821\u001b[0m fil \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(path_[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mswitch_punkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkers/maxent_ne_chunker\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m switch_chunker(fil\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:678\u001b[0m, in \u001b[0;36mswitch_punkt\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03mReturn a pickle-free Punkt tokenizer instead of loading a pickle.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m['Hello!', 'How are you?']\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PunktTokenizer \u001b[38;5;28;01mas\u001b[39;00m tok\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtok\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Charger manuellement punkt\n",
    "nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Tester `word_tokenize`\n",
    "try:\n",
    "    test_text = \"Bonjour, comment vas-tu aujourd'hui ?\"\n",
    "    print(\"🔹 Test de tokenization :\", word_tokenize(test_text))\n",
    "    print(\"✅ `word_tokenize` fonctionne correctement !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : `punkt` est toujours introuvable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ 'punkt' supprimé.\n",
      "✅ 'punkt' téléchargé avec succès !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "# Supprimer manuellement le dossier punkt\n",
    "punkt_path = os.path.expanduser(\"~/nltk_data/tokenizers/punkt\")\n",
    "shutil.rmtree(punkt_path, ignore_errors=True)\n",
    "print(\"🗑️ 'punkt' supprimé.\")\n",
    "\n",
    "# Télécharger à nouveau 'punkt'\n",
    "nltk.download(\"punkt\")\n",
    "print(\"✅ 'punkt' téléchargé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 'punkt' n'est toujours pas trouvé.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Chemin où `punkt` devrait être installé\n",
    "punkt_path = os.path.expanduser(\"~/nltk_data/tokenizers/punkt\")\n",
    "\n",
    "if os.path.exists(punkt_path):\n",
    "    print(f\"✅ 'punkt' est bien installé dans : {punkt_path}\")\n",
    "else:\n",
    "    print(\"❌ 'punkt' n'est toujours pas trouvé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ~/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ `punkt` téléchargé avec succès dans `~/nltk_data`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Télécharger `punkt` dans le bon chemin\n",
    "nltk.download(\"punkt\", download_dir=\"~/nltk_data\")\n",
    "\n",
    "print(\"✅ `punkt` téléchargé avec succès dans `~/nltk_data`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Chemins NLTK : ['/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/Nicolas/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ajouter explicitement le bon chemin\n",
    "nltk.data.path = [\"/Users/Nicolas/nltk_data\"] + nltk.data.path\n",
    "\n",
    "# Vérifier les chemins\n",
    "print(\"📌 Chemins NLTK :\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erreur : `punkt` est toujours introuvable.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    test_text = \"Bonjour, comment vas-tu aujourd'hui ?\"\n",
    "    print(\"🔹 Test de tokenization :\", word_tokenize(test_text))\n",
    "    print(\"✅ `word_tokenize` fonctionne correctement !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : `punkt` est toujours introuvable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp38-cp38-macosx_11_0_arm64.whl (284 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.8\n",
      "    Uninstalling click-8.1.8:\n",
      "      Successfully uninstalled click-8.1.8\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/Nicolas/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 'punkt' n'est toujours pas trouvé.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Vérifier si 'punkt' est bien installé\n",
    "punkt_path = os.path.expanduser(\"~/nltk_data/tokenizers/punkt\")\n",
    "\n",
    "if os.path.exists(punkt_path):\n",
    "    print(f\"✅ 'punkt' est bien installé dans : {punkt_path}\")\n",
    "else:\n",
    "    print(\"❌ 'punkt' n'est toujours pas trouvé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 'punkt' est introuvable dans les chemins connus.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Liste des chemins où `punkt` devrait être installé\n",
    "paths_to_check = [\n",
    "    \"/Users/Nicolas/nltk_data/tokenizers/punkt\",\n",
    "    \"/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data/tokenizers/punkt\",\n",
    "    \"/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data/tokenizers/punkt\"\n",
    "]\n",
    "\n",
    "# Vérifier si `punkt` est bien présent\n",
    "punkt_found = False\n",
    "for path in paths_to_check:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✅ 'punkt' est trouvé dans : {path}\")\n",
    "        punkt_found = True\n",
    "\n",
    "if not punkt_found:\n",
    "    print(\"❌ 'punkt' est introuvable dans les chemins connus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ `punkt` téléchargé avec succès dans `/Users/Nicolas/nltk_data`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Télécharger `punkt` dans un chemin forcé\n",
    "nltk.download(\"punkt\", download_dir=\"/Users/Nicolas/nltk_data\")\n",
    "\n",
    "print(\"✅ `punkt` téléchargé avec succès dans `/Users/Nicolas/nltk_data`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Chemins NLTK : ['/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/Nicolas/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ajouter explicitement le bon chemin\n",
    "nltk.data.path = [\"/Users/Nicolas/nltk_data\"] + nltk.data.path\n",
    "\n",
    "# Vérifier les chemins\n",
    "print(\"📌 Chemins NLTK :\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Charger `punkt` manuellement\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/english.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Tester `word_tokenize`\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:823\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    821\u001b[0m fil \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(path_[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mswitch_punkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkers/maxent_ne_chunker\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m switch_chunker(fil\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:678\u001b[0m, in \u001b[0;36mswitch_punkt\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03mReturn a pickle-free Punkt tokenizer instead of loading a pickle.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m['Hello!', 'How are you?']\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PunktTokenizer \u001b[38;5;28;01mas\u001b[39;00m tok\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtok\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.10/lib/python3.8/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data'\n    - '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Nicolas/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Charger `punkt` manuellement\n",
    "nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "# Tester `word_tokenize`\n",
    "try:\n",
    "    test_text = \"Bonjour, comment vas-tu aujourd'hui ?\"\n",
    "    print(\"🔹 Test de tokenization :\", word_tokenize(test_text))\n",
    "    print(\"✅ `word_tokenize` fonctionne correctement !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : `punkt` est toujours introuvable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Le dossier 'punkt' existe dans : /Users/Nicolas/nltk_data/tokenizers/punkt\n",
      "📂 Contenu du dossier :  ['greek.pickle', 'estonian.pickle', 'turkish.pickle', '.DS_Store', 'polish.pickle', 'PY3', 'russian.pickle', 'czech.pickle', 'portuguese.pickle', 'README', 'dutch.pickle', 'norwegian.pickle', 'malayalam.pickle', 'slovene.pickle', 'english.pickle', 'danish.pickle', 'finnish.pickle', 'swedish.pickle', 'spanish.pickle', 'german.pickle', 'italian.pickle', 'french.pickle']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "punkt_path = \"/Users/Nicolas/nltk_data/tokenizers/punkt\"\n",
    "\n",
    "if os.path.exists(punkt_path):\n",
    "    print(f\"✅ Le dossier 'punkt' existe dans : {punkt_path}\")\n",
    "    print(\"📂 Contenu du dossier : \", os.listdir(punkt_path))\n",
    "else:\n",
    "    print(\"❌ Le dossier 'punkt' est introuvable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Chemins de recherche NLTK : ['/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/share/nltk_data', '/Users/Nicolas/.pyenv/versions/3.8.10/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/Nicolas/nltk_data', '/Users/Nicolas/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ajouter le bon chemin à la liste de recherche\n",
    "nltk.data.path.append(\"/Users/Nicolas/nltk_data\")\n",
    "\n",
    "# Vérifier les chemins de NLTK\n",
    "print(\"📌 Chemins de recherche NLTK :\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erreur : `punkt` est toujours introuvable.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    test_text = \"Bonjour, comment vas-tu aujourd'hui ?\"\n",
    "    print(\"🔹 Test de tokenization :\", word_tokenize(test_text))\n",
    "    print(\"✅ `word_tokenize` fonctionne correctement !\")\n",
    "except LookupError:\n",
    "    print(\"❌ Erreur : `punkt` est toujours introuvable.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
